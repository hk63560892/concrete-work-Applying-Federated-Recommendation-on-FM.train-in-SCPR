{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d6dfce-ddb1-45dd-a39e-f1fda09fb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import *\n",
    "from Graph_generate.lastfm_data_process import LastFmDataset\n",
    "from Graph_generate.lastfm_star_data_process import LastFmStarDataset\n",
    "from Graph_generate.lastfm_graph import LastFmGraph\n",
    "from Graph_generate.yelp_data_process import YelpDataset\n",
    "from Graph_generate.yelp_graph import YelpGraph\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "#from data import load_dataset\n",
    "\n",
    "#from FedRec.server import FedRecServer\n",
    "#from FedRec.client import FedRecClient\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pickle\n",
    "from utils import * \n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from FM.FM_model import FactorizationMachine\n",
    "from FM.FM_feature_evaluate import evaluate_feature\n",
    "from FM.FM_item_evaluate import evaluate_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5e1e52-ced6-4393-8bef-c1a770297297",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722cfdff-15de-46c8-9e61-0f7413192b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=None\n",
    "epochs=200\n",
    "batch_size=256\n",
    "grad_limit=1.0\n",
    "clients_limit=0.05\n",
    "items_limit=60\n",
    "part_percent=1\n",
    "attack_lr=0.01\n",
    "attack_batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f1edad-c3ca-4c5d-95b0-51278de6f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCPR agrs\n",
    "lr=0.02\n",
    "flr=0.0001\n",
    "reg=0.001\n",
    "decay=0.0\n",
    "qonly=1\n",
    "bs=64\n",
    "hs=64\n",
    "ip=0.01\n",
    "dr=0.5\n",
    "optim=\"Ada\"\n",
    "observe=25\n",
    "uf=1\n",
    "rd=0\n",
    "useremb=1\n",
    "freeze=0\n",
    "command=8\n",
    "seed=0\n",
    "max_epoch=250\n",
    "pretrain=0\n",
    "load_fm_epoch=0\n",
    "#data_name=LAST_FM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e044e34-7d64-4585-9289-7798c60a4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name=\"LAST_FM_STAR\"\n",
    "dataset = load_dataset(data_name)\n",
    "ITEM = 'item'\n",
    "ITEM_FEATURE = 'belong_to'\n",
    "\n",
    "dataset = load_dataset(data_name)\n",
    "kg = load_kg(data_name)\n",
    "hs=64\n",
    "qonly=1\n",
    "ip=0.01\n",
    "dr=0.5\n",
    "user_length, item_length, feature_length=int(getattr(dataset, 'user').value_len),int(getattr(dataset, 'item').value_len),int(getattr(dataset, 'feature').value_len)\n",
    "bs = 64\n",
    "max_epoch=250\n",
    "lr = 0.02\n",
    "decay=0.0\n",
    "flr=0.0001\n",
    "reg=0.001\n",
    "observe=25\n",
    "command=8\n",
    "uf=1\n",
    "seed=0\n",
    "useremb=1\n",
    "load_fm_epoch=0\n",
    "PAD_IDX1 = user_length + item_length\n",
    "PAD_IDX2 = feature_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657e682d-4bf8-4615-b75b-bdbf579d87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getattr(dataset, 'item')#從0~7431，總共7432個\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0019db-458f-4157-a588-297b1170f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getattr(dataset, 'feature')#從0~8437，總共8438個\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2adfa2d8-f7be-4cc4-9c2d-61ba4b01eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getattr(dataset, 'user')#0~1800 總共1801個uid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a7235-e391-4d0e-a22a-58c6f341a66f",
   "metadata": {},
   "source": [
    "# initialize KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c177e7-3ac9-4690-9679-3016328b0f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " DatasetDict = {\n",
    "        LAST_FM: LastFmDataset,\n",
    "        LAST_FM_STAR: LastFmStarDataset,#here\n",
    "        YELP: YelpDataset,\n",
    "        YELP_STAR: YelpDataset\n",
    "    }\n",
    "GraphDict = {\n",
    "        LAST_FM: LastFmGraph,\n",
    "        LAST_FM_STAR: LastFmGraph,\n",
    "        YELP: YelpGraph,\n",
    "        YELP_STAR: YelpGraph\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f121584-2abf-486c-96bf-3250d47b73b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/lastfm_star'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from easydict import EasyDict as edict\n",
    "data_dir =\"./data/lastfm_star\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a169f7fd-b96f-4d4d-950b-296df6e5271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastFmStarDataset(object):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir + '/Graph_generate_data'\n",
    "        self.load_entities()\n",
    "        self.load_relations()\n",
    "    def get_relation(self):\n",
    "        # Entities\n",
    "        USER = 'user'\n",
    "        ITEM = 'item'\n",
    "        FEATURE = 'feature'\n",
    "\n",
    "        # Relations\n",
    "        INTERACT = 'interact'\n",
    "        FRIEND = 'friends'\n",
    "        LIKE = 'like'\n",
    "        BELONG_TO = 'belong_to'\n",
    "        relation_name = [INTERACT, FRIEND, LIKE, BELONG_TO]\n",
    "\n",
    "        fm_relation = { \n",
    "            USER: {#user 这个entity有三个relation，分别relation什么类型的entity\n",
    "                INTERACT: ITEM,\n",
    "                FRIEND: USER,\n",
    "                LIKE: FEATURE,\n",
    "            },\n",
    "            ITEM: {\n",
    "                BELONG_TO: FEATURE,\n",
    "                INTERACT: USER\n",
    "            },\n",
    "            FEATURE: {\n",
    "                LIKE: USER,\n",
    "                BELONG_TO: ITEM\n",
    "            }\n",
    "        }\n",
    "        fm_relation_link_entity_type = {#the keys are the relationship types and the values are lists representing the source entity and target entity respectively\n",
    "            INTERACT: [USER, ITEM],\n",
    "            FRIEND: [USER, USER],\n",
    "            LIKE: [USER, FEATURE],\n",
    "            BELONG_TO: [ITEM, FEATURE]\n",
    "        }\n",
    "        return fm_relation, relation_name, fm_relation_link_entity_type\n",
    "\n",
    "        \n",
    "    def load_entities(self):\n",
    "        #load entities with LAST_FM_STAR DATA\n",
    "        entity_files = edict(\n",
    "                    user='user_dict.json',\n",
    "                    item='item_dict.json',\n",
    "                    feature='original_tag_map.json',\n",
    "                )\n",
    "        for entity_name in entity_files:\n",
    "            with open(os.path.join(self.data_dir, entity_files[entity_name]), encoding='utf-8') as f:\n",
    "                mydict = json.load(f)\n",
    "                #user_dict\n",
    "                #friends refers to who are friends with the user .\n",
    "                #\"like refers to the item that user like\"\n",
    "                #the format:{\"0\":{\"friends\":[246, 382, 462, 676, 735],\"like\":[11,12,13,14,15]},\"1\":...}\n",
    "\n",
    "                #if entity_name == 'item':#{'0': {'feature_index': [69, 75]}, '1': {'feature_index': [74, 77, 80, 139, 706]}\n",
    "                    #print(mydict)\n",
    "\n",
    "                if entity_name == 'feature':#Each feature is assigned a unique index\n",
    "                    #print(mydict){'1': 0, '2': 1, '3': 2, '4': 3, '5': 4,...Each feature is assigned a unique index\n",
    "                    entity_id = list(mydict.values())#[0, 1, 2, 3, 4, 5,... list of index\n",
    "\n",
    "                else:\n",
    "                    #uid's attribute {'id': [0, 1, 2, 3, 4, 5, 6, 7,...]'value_len': 1801}\n",
    "                    #iid\"s attribute: {'id': [0, 1, 2, 3, 4,...],'value_len': 7432}\n",
    "                    #feature's attribute :{'id': [0, 1, 2, 3, 4,...],'value_len': 8438}\n",
    "                    entity_id = list(map(int, list(mydict.keys())))#[0, 1, 2, 3, 4, 5, ...]\n",
    "                setattr(self,entity_name, edict(id=entity_id, value_len=max(entity_id) + 1))\n",
    "                    #print(getattr(self, entity_name))\n",
    "                print('Load', entity_name, 'of size', len(entity_id))\n",
    "                print(entity_name, 'of max id is', max(entity_id))\n",
    "                \n",
    "    #用entity来建立relation\n",
    "    def load_relations(self):\n",
    "        \"\"\"\n",
    "        relation: head entity---> tail entity\n",
    "        --\n",
    "        \"\"\"\n",
    "        LastFm_relations = edict(#根据user_item_train.json 这个file进行配对\n",
    "            interact=('user_item_train.json', self.user, self.item),  # (filename, head_entity, tail_entity) self.user and self.item is uid and iid attribute\n",
    "            friends=('user_dict.json', self.user, self.user),\n",
    "            like=('user_dict.json', self.user, self.feature),\n",
    "            belong_to=('item_dict.json', self.item, self.feature),\n",
    "        )\n",
    "        for name in LastFm_relations:\n",
    "            #  Save tail_entity\n",
    "            relation = edict(\n",
    "                data=[],\n",
    "            )\n",
    "            #print(relation)\n",
    "            #{'data': []}\n",
    "\n",
    "            knowledge = [list([]) for i in range(LastFm_relations[name][1].value_len)] #create many list as the size of head_entity\n",
    "            with open(os.path.join(self.data_dir, LastFm_relations[name][0]), encoding='utf-8') as f:\n",
    "                mydict = json.load(f)\n",
    "                if name in ['interact']:#interact=('user_item_train.json', self.user, self.item)\n",
    "                    #print(mydict) {'0': [5780, 5781, 5782, 5783, ...],\"1\":[154, 155, 156, 157,...],...}#user_item interaction for train data\n",
    "                    for key, value in mydict.items():\n",
    "                        head_id = int(key)\n",
    "                        tail_ids = value\n",
    "                        knowledge[head_id] = tail_ids#knowledge[0] 用index来代表 head user id，value是[item id]来代表interaction\n",
    "                elif name in ['friends']:#user 的friends是什么\n",
    "                    #print(mydict) the data from friends=('user_dict.json' \n",
    "                    #from friends like:   {'0': {'friends': [246, 382, 462, ...],\"like\":[11, 12, 13, 14,...]},'1':...}\n",
    "                    for key in mydict.keys():\n",
    "                        head_str = key\n",
    "                        head_id = int(key)\n",
    "                        tail_ids = mydict[head_str][name]\n",
    "                        knowledge[head_id] = tail_ids #index 是user head id ，value是user tail id（user head 的friends）\n",
    "                elif name in ['like']:#user 喜欢什么feature/attribute\n",
    "                    #print(mydict) #the data  from  like=('user_dict.json'\n",
    "                    #from like:   {'0': {'friends': [246, 382, 462, ...],\"like\":[11, 12, 13, 14,...]},'1':...}\n",
    "                    for key in mydict.keys():\n",
    "                        head_str = key\n",
    "                        head_id = int(key)\n",
    "                        tail_ids = mydict[head_str][name]\n",
    "                        knowledge[head_id] = tail_ids #index 是user head id ，value是user tail id（user head like的attribute）\n",
    "                elif name in ['belong_to']:#该item belongs to 什么feature\n",
    "                    #print(mydict)#from {'0': {'feature_index': [69, 75]}, '1': {'feature_index': [74, 77, 80, 139, 706]},\n",
    "                    for key in mydict.keys():\n",
    "                        head_str = key\n",
    "                        head_id = int(key)\n",
    "                        tail_ids = mydict[head_str]['feature_index']\n",
    "                        knowledge[head_id] = tail_ids#index 是item head id ，value是feature tail id\n",
    "                    \n",
    "                relation.data = knowledge\n",
    "                setattr(self, name, relation) #for example self.interact includes the {'data':[knowledge of interact]}\n",
    "                tuple_num = 0\n",
    "                for i in knowledge:\n",
    "                    tuple_num += len(i)\n",
    "                print('Load', name, 'of size', tuple_num)\n",
    "            #now we have 4 attributes contains the relations of each entity\n",
    "            #print(self.interact)   {'data': [[5780, 5781, 5782, 5783, ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa920565-32fe-43d0-920d-06c92c158b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name=\"LAST_FM_STAR\"\n",
    "dataset = load_dataset(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183a34a1-40b4-4dac-8038-6eaf0c1f4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load entities...\n",
      "load entity:user  : Total 1801 nodes.\n",
      "load entity:item  : Total 7432 nodes.\n",
      "load entity:feature  : Total 8438 nodes.\n",
      "ALL total 17671 nodes.\n",
      "===============END==============\n",
      "Load knowledge interact...\n",
      "Total 105820 interact edges.\n",
      "Load knowledge friends...\n",
      "Total 47916 friends edges.\n",
      "Load knowledge like...\n",
      "Total 66240 like edges.\n",
      "Load knowledge belong_to...\n",
      "Total 188892 belong_to edges.\n",
      "===============END==============\n",
      "Remove duplicates...\n"
     ]
    }
   ],
   "source": [
    "kg = GraphDict[data_name](dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5500d44-fca9-4ae6-9309-868d64acf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastFmGraph(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.G = dict()\n",
    "        self._load_entities(dataset)\n",
    "        self._load_knowledge(dataset)\n",
    "        self._clean()\n",
    "    def _load_entities(self, dataset):\n",
    "        print('load entities...')\n",
    "        num_nodes = 0\n",
    "        #dataset is a instance of  LastFmStarDataset in the file lastfm_star_data_process.py\n",
    "        data_relations, _, _ = dataset.get_relation()  # entity_relations, relation_name, link_entity_type\n",
    "        #print(data_relations)# how head entity relate to tail entity {'user': {'interact': 'item', 'friends': 'user', 'like': 'feature'}, 'item': {'belong_to': 'feature', 'interact': 'user'}, 'feature': {'like': 'user', 'belong_to': 'item'}}\n",
    "        entity_list = list(data_relations.keys())#user item feature\n",
    "        for entity in entity_list:\n",
    "            self.G[entity] = {}# loop 完三次后：{'user': {}, 'item': {}, 'feature': {}}\n",
    "            entity_size = getattr(dataset, entity).value_len #等于self.user.value_len\n",
    "\n",
    "            for eid in range(entity_size):\n",
    "                entity_rela_list = data_relations[entity].keys()\n",
    "                self.G[entity][eid] = {r: [] for r in entity_rela_list}\n",
    "            #print(self.G) if entity is user:{'user': {0: {'interact': [], 'friends': [], 'like': []},1:...第一个dic的key是 entity type，value 是entity id，entity id的value则是relation type,relation type 的value是relation entities\n",
    "            num_nodes += entity_size\n",
    "            print('load entity:{:s}  : Total {:d} nodes.'.format(entity, entity_size))\n",
    "        print('ALL total {:d} nodes.'.format(num_nodes))\n",
    "        print('===============END==============')\n",
    "        #这里是每个entity的{'user': {0: {'interact': [], 'friends': [], 'like': []},1:...},'item': {0: {'belong_to': [], 'interact': []}, 1:...},'feature': {0: {'like': [], 'belong_to': []}, 1: ...}}\n",
    "        #print(self.G)\n",
    "    def _load_knowledge(self, dataset):\n",
    "        #data_relations_name = relation_name = [INTERACT, FRIEND, LIKE, BELONG_TO]\n",
    "       #link_entity_type=fm_relation_link_entity_type = {     #the keys are the relationship types and the values are lists representing the source entity and target entity respectively\n",
    "            #INTERACT: [USER, ITEM],\n",
    "            #FRIEND: [USER, USER],\n",
    "            #LIKE: [USER, FEATURE],\n",
    "           # BELONG_TO: [ITEM, FEATURE]}\n",
    "        \n",
    "        _, data_relations_name, link_entity_type = dataset.get_relation()\n",
    "        for relation in data_relations_name:\n",
    "            print('Load knowledge {}...'.format(relation))\n",
    "            data = getattr(dataset, relation).data\n",
    "            num_edges = 0\n",
    "            for he_id, te_ids in enumerate(data):# head_entity_id 取出他的index，也就是head id, tail_entity_ids\n",
    "                if len(te_ids) <= 0:\n",
    "                    continue\n",
    "                e_head_type = link_entity_type[relation][0] #找出link_entity_type中的head type and tail type :{user item feature}\n",
    "                e_tail_type = link_entity_type[relation][1]\n",
    "                for te_id in set(te_ids):\n",
    "                    self._add_edge(e_head_type, he_id, relation, e_tail_type, te_id)#填入这些slot{'user': {0: {'interact': [], 'friends': [], 'like': []},1:...},'item': {0: {'belong_to': [], 'interact': []}, 1:...},'feature': {0: {'like': [], 'belong_to': []}, 1: ...}}\n",
    "                    num_edges += 2\n",
    "            print('Total {:d} {:s} edges.'.format(num_edges, relation))\n",
    "        print('===============END==============')\n",
    "        #KG最终形态！！！！！！！！print(self.G)#{'user': {0: {'interact': [6663, 6664, 6666, 6667, \n",
    "\n",
    "                \n",
    "    def _add_edge(self, etype1, eid1, relation, etype2, eid2):\n",
    "        self.G[etype1][eid1][relation].append(eid2)\n",
    "        self.G[etype2][eid2][relation].append(eid1)\n",
    "    def _clean(self):\n",
    "        print('Remove duplicates...')\n",
    "        for etype in self.G:\n",
    "            for eid in self.G[etype]:\n",
    "                for r in self.G[etype][eid]:\n",
    "                    data = self.G[etype][eid][r]\n",
    "                    data = tuple(sorted(set(data)))\n",
    "                    self.G[etype][eid][r] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9156d2-0363-433b-b422-7801abfdaf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load entities...\n",
      "load entity:user  : Total 1801 nodes.\n",
      "load entity:item  : Total 7432 nodes.\n",
      "load entity:feature  : Total 8438 nodes.\n",
      "ALL total 17671 nodes.\n",
      "===============END==============\n",
      "Load knowledge interact...\n",
      "Total 105820 interact edges.\n",
      "Load knowledge friends...\n",
      "Total 47916 friends edges.\n",
      "Load knowledge like...\n",
      "Total 66240 like edges.\n",
      "Load knowledge belong_to...\n",
      "Total 188892 belong_to edges.\n",
      "===============END==============\n",
      "Remove duplicates...\n"
     ]
    }
   ],
   "source": [
    "abc=LastFmGraph(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14471753-254d-44aa-958f-1829bc402eca",
   "metadata": {},
   "source": [
    "# RecServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c47a52-5f8b-4cb2-a0f9-2751b793bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedRecServer(nn.Module):#initialized item embedding\n",
    "    def __init__(self, emb_size, user_length, item_length, feature_length, qonly, hs, ip, dr):#3706,32\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_length = user_length#get from global\n",
    "        self.item_length  = item_length\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "        self.hs = hs\n",
    "        self.ip = ip\n",
    "        self.dr = dr\n",
    "        \n",
    "        #不用\n",
    "        self.qonly = qonly  # only use quadratic form\n",
    "        \n",
    "\n",
    "        # dimensions\n",
    "        self.emb_size = emb_size\n",
    "        self.items_emb = nn.Embedding(self.item_length, self.hs+1).to(device)\n",
    "        nn.init.normal_(self.items_emb.weight, std=0.01)\n",
    "        \n",
    "        self.feature_emb = nn.Embedding(self.feature_length+1, self.hs + 1, padding_idx=self.feature_length, sparse=False).to(device)\n",
    "        self.feature_emb.weight.data.normal_(0,self.ip)\n",
    "\n",
    "        # _______ set the padding to zero _______\n",
    "        self.feature_emb.weight.data[feature_length,:] = 0\n",
    "        \n",
    "\n",
    "        \n",
    "    def train_(self, client,pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_new, index_none, residual_feature, neg_feature):#clients以及他们的index放进去\n",
    "        \n",
    "        self.pos_list = pos_list\n",
    "        self.pos_list2 = pos_list2\n",
    "        self.neg_list = neg_list\n",
    "        self.neg_list2 = neg_list2\n",
    "        self.new_neg_list=new_neg_list\n",
    "        self.new_neg_list2=new_neg_list2\n",
    "        self.preference_list_1=preference_list_1\n",
    "        self.preference_list_new = preference_list_new\n",
    "        self.index_none=index_none\n",
    "        self.residual_feature=residual_feature\n",
    "        self.neg_feature=neg_feature\n",
    "\n",
    "                \n",
    "        #items_emb_grad,feature_emb_grad, loss, loss_2=client.train_(self.items_emb,self.feature_emb,self.pos_list, self.pos_list2, self.neg_list, self.neg_list2, self.new_neg_list, self.new_neg_list2, self.preference_list_1, self.preference_list_new, self.index_none, self.residual_feature, self.neg_feature)\n",
    "        batch_items_emb_grad = torch.zeros_like(self.items_emb.weight)#initialize the gradient value of item embedding\n",
    "        batch_feature_emb_grad = torch.zeros_like(self.feature_emb.weight)\n",
    "        \n",
    "        for i in range(len(pos_list)):\n",
    "            user_id = pos_list[i][0].item()\n",
    "            items_emb_grad,feature_emb_grad, loss, loss_2=client[user_id].train_(self.items_emb,self.feature_emb,self.pos_list[i], self.pos_list2[i], self.neg_list[i], self.neg_list2[i], self.new_neg_list[i], self.new_neg_list2[i], self.preference_list_1[i], self.preference_list_new[i], self.index_none, self.residual_feature[i], self.neg_feature[i])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                norm = items_emb_grad.norm(2, dim=-1, keepdim=True)\n",
    "                too_large = norm[:, 0] > grad_limit#1\n",
    "                items_emb_grad[too_large] /= (norm[too_large] / grad_limit)#gradient clipping step\n",
    "                batch_items_emb_grad += items_emb_grad\n",
    "\n",
    "                #if optim == 'Ada':\n",
    "                    #optimizer3 = torch.optim.Adagrad([items_emb_weight], lr=lr, weight_decay=decay)\n",
    "                    #optimizer3.step()\n",
    "\n",
    "                #self.items_emb.weight=items_emb_weight\n",
    "\n",
    "                norm1 = feature_emb_grad.norm(2, dim=-1, keepdim=True)\n",
    "                too_large1 = norm1[:, 0] > grad_limit#1\n",
    "                feature_emb_grad[too_large1] /= (norm1[too_large1] / grad_limit)#gradient clipping step\n",
    "                batch_feature_emb_grad += feature_emb_grad\n",
    "\n",
    "                #if optim == 'Ada':\n",
    "                    #optimizer4 = torch.optim.Adagrad([feature_emb_weight], lr=lr, weight_decay=decay)\n",
    "                    #optimizer4.step()\n",
    "\n",
    "                #self.feature_emb.weight=feature_emb_weight\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.items_emb.weight.data.add_(batch_items_emb_grad, alpha=-lr)#update 最后的item embedding weight\n",
    "            self.feature_emb.weight.data.add_(batch_feature_emb_grad, alpha=-lr)\n",
    "        return loss,loss_2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03465bbe-ace5-459d-9a97-da2371d57818",
   "metadata": {},
   "source": [
    "# FedClients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d08c6e5-cf06-49fd-9234-cc9b4f5ed7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FedRecClient(nn.Module):#为了学好user embedding（但只储存在此处），item embedding的weight可以上传server\n",
    "    def __init__(self,emb_size, user_length, item_length, feature_length, qonly, hs, ip, dr):\n",
    "        super(FedRecClient, self).__init__()#super().__init__()\n",
    "              \n",
    "        self.items_emb_grad = None\n",
    "        \n",
    "        self.model=FactorizationMachine(emb_size=hs, user_length=user_length, item_length=item_length,feature_length=feature_length, qonly=qonly, hs=hs, ip=ip, dr=dr).to(device)\n",
    "\n",
    "   \n",
    "    \n",
    "    def train_(self,items_emb,feature_emb,pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_new, index_none, residual_feature, neg_feature):#要修改compute，在最后要把item embedding 独立出来，然后再return给server\n",
    "        self.pos_list = pos_list\n",
    "        self.pos_list2 = pos_list2\n",
    "        self.neg_list = neg_list\n",
    "        self.neg_list2 = neg_list2\n",
    "        self.new_neg_list=new_neg_list\n",
    "        self.new_neg_list2=new_neg_list2\n",
    "        self.preference_list_1=preference_list_1\n",
    "        self.preference_list_new = preference_list_new\n",
    "        self.index_none=index_none\n",
    "        self.residual_feature=residual_feature\n",
    "        self.neg_feature=neg_feature  \n",
    "        #optimizer\n",
    "        i=0\n",
    "        param1, param2 = list(), list()\n",
    "        for name, param in self.model.named_parameters():\n",
    "                #print(name, param)\n",
    "                if i == 0:\n",
    "                    param1.append(param)\n",
    "                else:\n",
    "                    param2.append(param)\n",
    "                i += 1\n",
    "        \n",
    "        if optim == 'Ada':\n",
    "                optimizer1 = torch.optim.Adagrad(param1, lr=lr, weight_decay=decay)\n",
    "                optimizer2 = torch.optim.Adagrad(param2, lr=lr, weight_decay=decay)\n",
    "                \n",
    "        self.items_emb=items_emb\n",
    "        self.feature_emb=feature_emb\n",
    "        \n",
    "        #reset the gradient\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "\n",
    "        result_pos, feature_bias_matrix_pos, nonzero_matrix_pos = self.model(self.items_emb,self.feature_emb,self.pos_list.unsqueeze(0),self.pos_list2.unsqueeze(0),self.preference_list_1.unsqueeze(0))  # (bs, 1), (bs, 2, 1), (bs, 2, emb_size)\n",
    "\n",
    "        result_neg, feature_bias_matrix_neg, nonzero_matrix_neg = self.model(self.items_emb,self.feature_emb,self.neg_list.unsqueeze(0), self.neg_list2.unsqueeze(0), self.preference_list_1.unsqueeze(0))\n",
    "        if self.model.items_emb.weight.grad is not None:\n",
    "            self.model.items_emb.weight.grad.zero_()\n",
    "\n",
    "        diff = (result_pos - result_neg)\n",
    "        loss = - lsigmoid(diff).sum(dim=0)\n",
    "        \n",
    "        if command in [8]:\n",
    "                # The second type of negative sample\n",
    "                new_result_neg, new_feature_bias_matrix_neg, new_nonzero_matrix_neg = self.model(self.items_emb,self.feature_emb,self.new_neg_list.unsqueeze(0), self.new_neg_list2.unsqueeze(0),\n",
    "                                                                                            self.preference_list_new.unsqueeze(0))\n",
    "                # Reason for this is that, sometimes the sample is missing, so we have to also omit that in result_pos\n",
    "                T = cuda_(torch.tensor([]))\n",
    "                for i in range(1):\n",
    "                    if i in index_none:\n",
    "                        continue\n",
    "                    T = torch.cat([T, result_pos[i]], dim=0)\n",
    "\n",
    "                T = T.view(T.shape[0], -1)\n",
    "                assert T.shape[0] == new_result_neg.shape[0]\n",
    "                diff = T - new_result_neg\n",
    "                if loss is not None:\n",
    "                    loss += - lsigmoid(diff).sum(dim=0)\n",
    "                else:\n",
    "                    loss = - lsigmoid(diff).sum(dim=0)\n",
    "        \n",
    "        \n",
    "        # regularization\n",
    "        if reg_float != 0:\n",
    "                if qonly != 1:\n",
    "                    feature_bias_matrix_pos_ = (feature_bias_matrix_pos ** 2).sum(dim=1)  # (bs, 1)\n",
    "                    feature_bias_matrix_neg_ = (feature_bias_matrix_neg ** 2).sum(dim=1)  # (bs, 1)\n",
    "                    nonzero_matrix_pos_ = (nonzero_matrix_pos ** 2).sum(dim=2).sum(dim=1, keepdim=True)  # (bs, 1)\n",
    "                    nonzero_matrix_neg_ = (nonzero_matrix_neg ** 2).sum(dim=2).sum(dim=1, keepdim=True)  # (bs, 1)\n",
    "                    new_nonzero_matrix_neg_ = (new_nonzero_matrix_neg_ ** 2).sum(dim=2).sum(dim=1, keepdim=True)\n",
    "                    regular_norm = (\n",
    "                                feature_bias_matrix_pos_ + feature_bias_matrix_neg_ + nonzero_matrix_pos_ + nonzero_matrix_neg_ + new_nonzero_matrix_neg_)\n",
    "                    loss += (reg * regular_norm).sum(dim=0)\n",
    "                else:\n",
    "                    nonzero_matrix_pos_ = (nonzero_matrix_pos ** 2).sum(dim=2).sum(dim=1, keepdim=True)\n",
    "                    nonzero_matrix_neg_ = (nonzero_matrix_neg ** 2).sum(dim=2).sum(dim=1, keepdim=True)\n",
    "                    loss += (reg * nonzero_matrix_pos_).sum(dim=0)\n",
    "                    loss += (reg * nonzero_matrix_neg_).sum(dim=0)\n",
    "        \n",
    "      \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        #if optim == 'Ada':\n",
    "            #optimizer3 = torch.optim.Adagrad([self.model.items_emb.weight], lr=lr, weight_decay=decay)\n",
    "        #optimizer3.step()\n",
    "        \n",
    "        #update user emb \n",
    "        #user_emb_grad = self.model.user_emb.weight.grad # extracts the gradient computed for the user embeddings during the backward pass\n",
    "        #self.model.user_emb.weight.data.add_(user_emb_grad, alpha=-lr)#updates the user embeddings using gradient descent\n",
    "        \n",
    "        #update bias\n",
    "        #bias_grad = self.model.Bias.grad\n",
    "        #self.model.Bias.data.add_(bias_grad, alpha=-lr)\n",
    "        \n",
    "            \n",
    "        self.items_emb_grad = self.model.items_emb.weight.grad# stores the gradient computed for the item embeddings into the self.items_emb_grad attribute of the client\n",
    "        \n",
    "        loss_2=0\n",
    "        \n",
    "        if uf == 1:\n",
    "                # updating feature embedding\n",
    "                # we try to optimize\n",
    "                A = self.model.feature_emb(preference_list_1[0].unsqueeze(0)).unsqueeze(0)[..., :-1]\n",
    "                #print(\"A Shape\",A.shape)\n",
    "                #print(\"=================================================================\")\n",
    "                user_emb = self.model.ui_emb[0][0].unsqueeze(0)[..., :-1].unsqueeze(dim=1).detach()\n",
    "                #print(\"ui_emb_emb(pos_list[:, 0])\",self.model.ui_emb[0][0].unsqueeze(0).shape)\n",
    "                #print(\"model.ui_emb(pos_list[:, 0])[..., :-1].shape:\",self.model.ui_emb[0][0].unsqueeze(0)[..., :-1].shape)\n",
    "                #print(\"user_emb.shape\",user_emb.shape)\n",
    "                #print(\"=================================================================\")\n",
    "                if useremb == 1:\n",
    "                    A = torch.cat([A, user_emb], dim=1)\n",
    "\n",
    "                B = self.model.feature_emb(residual_feature.unsqueeze(0))[..., :-1]\n",
    "                C = self.model.feature_emb(neg_feature.unsqueeze(0))[..., :-1]\n",
    "\n",
    "                D = torch.matmul(A, B.transpose(2, 1))\n",
    "                E = torch.matmul(A, C.transpose(2, 1))\n",
    "\n",
    "                p_vs_residual = D.view(D.shape[0], -1, 1)\n",
    "                p_vs_neg = E.view(E.shape[0], -1, 1)\n",
    "\n",
    "                p_vs_residual = p_vs_residual.sum(dim=1)\n",
    "                p_vs_neg = p_vs_neg.sum(dim=1)\n",
    "                diff = (p_vs_residual - p_vs_neg)\n",
    "                temp = - lsigmoid(diff).sum(dim=0)\n",
    "                loss = temp\n",
    "                loss_2 += temp.data\n",
    "\n",
    "                if self.model.feature_emb.weight.grad is not None:\n",
    "                    self.model.feature_emb.weight.grad.zero_()\n",
    "\n",
    "           \n",
    "                loss.backward()\n",
    "                #if optim == 'Ada':\n",
    "                    #optimizer4 = torch.optim.Adagrad([self.model.feature_emb.weight], lr=flr, weight_decay=decay)\n",
    "                    #optimizer4.step()\n",
    "                self.feature_emb_grad = self.model.feature_emb.weight.grad#\n",
    "        #print(loss)    \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        return self.items_emb_grad,self.feature_emb_grad,loss.cpu().item(),loss_2.cpu().item()\n",
    "\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc34e39-24d3-4c04-9cde-b4e5c064dcb0",
   "metadata": {},
   "source": [
    "# FM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d0ef84-a83e-4ebe-afbc-84b5a3f37f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FactorizationMachine(nn.Module):#In general, a second-order Factorization Machine models the interaction between every pair of features in the input data.\n",
    "    \"\"\" the quadratic form refers to the second order interaction between features that is captured by the model. Factorization Machines capture both linear relationships (first order) and interactions between pairs of features (second order or quadratic).\n",
    "\n",
    "Setting qonly to True would mean that the model only considers these second order or quadratic interactions, ignoring the first order linear relationships.\n",
    "\n",
    "Why might you want to do this? It depends on the specific problem and dataset. In some situations, the interactions between features might be much more important than the individual linear effects. For instance, in recommendation systems, the interaction between a specific user and a specific item (user-item interaction) could be more important than the effect of the user or item individually.\"\"\"\n",
    "    def __init__(self,emb_size, user_length, item_length, feature_length, qonly, hs, ip, dr):\n",
    "\n",
    "        super(FactorizationMachine, self).__init__()#super().__init__()\n",
    "        \n",
    "\n",
    "        self.user_length = user_length#get from global\n",
    "        self.item_length  = item_length\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "        self.hs = hs\n",
    "        self.ip = ip\n",
    "        self.dr = dr\n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=self.dr)  # dropout ratio\n",
    "        self.qonly = qonly  # only use quadratic form\n",
    "        \n",
    "\n",
    "        # dimensions\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        # _______ User embedding + Item embedding\n",
    "        self.user_emb = nn.Embedding(1,hs + 1, sparse=False).to(device)#user_length + item_length + 1 is the number of unique users and items,This extra dimension is used to store the bias term for each embedding. \n",
    "        \n",
    "        \n",
    "        \n",
    "        # _______ Feature embedding and Preference embedding are common_______\n",
    "        #self.feature_emb = nn.Embedding(self.feature_length + 1, emb_size + 1, padding_idx=self.feature_length, sparse=False)\n",
    "\n",
    "        # _______ Scala Bias _______\n",
    "        self.Bias = nn.Parameter(torch.randn(1).normal_(0, 0.01), requires_grad=True)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "        \n",
    "\n",
    "    def init_weight(self):\n",
    "        self.user_emb.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    '''\n",
    "        param: a list of user ID and busi ID\n",
    "        '''\n",
    "    def forward(self,items_emb,feature_emb,ui_pair, feature_index, preference_index):\n",
    "        self.items_emb=items_emb\n",
    "        self.feature_emb=feature_emb\n",
    "        user_idx = torch.tensor([0]).to(device)#doesnt matter每個client都是獨立的\n",
    "        self.real_idx=ui_pair[0][1]-user_length\n",
    "        self.ui_emb = torch.cat([self.user_emb(user_idx), self.items_emb(self.real_idx).unsqueeze(0)],dim=0).to(device).unsqueeze(0)\n",
    "        \n",
    "        #ui_emb 要有+1 ，而不是item_emb\n",
    "        feature_matrix_ui = self.ui_emb#将pos_list做uid 跟iid的embedding\n",
    "\n",
    "        nonzero_matrix_ui = feature_matrix_ui[..., :-1]#This line is taking all dimensions except for the last one. \n",
    "        feature_bias_matrix_ui = feature_matrix_ui[..., -1:]#This line is taking just the last dimension. The -1: in indexing means \"get the last dimension only\".the bias term\n",
    "#\"\"\"bias terms for each pair.\n",
    "#tensor([[[ 5.8801e-03],\n",
    "         #[ 2.0062e-02]],\n",
    "\n",
    "        #[[ 6.4298e-04],\n",
    "         #[-1.0485e-02]],\n",
    "\n",
    "        ##[[ 3.6053e-03],\n",
    "        # [ 8.7155e-03]],\n",
    "\n",
    "\n",
    "        feature_matrix_preference = self.feature_emb(preference_index)#get preference embedding\n",
    "        # _______ dropout has been done already (when data was passed in) _______\n",
    "        nonzero_matrix_preference = feature_matrix_preference[..., :-1]  # (bs, 2, emb_size)\n",
    "        feature_bias_matrix_preference = feature_matrix_preference[..., -1:]  # (bs, 2, 1)\n",
    "        #\"\"\"feature bias:\n",
    "        #tensor([[[ 0.0033],\n",
    "         #[ 0.0119],\n",
    "         #[ 0.0077],\n",
    "\n",
    "\n",
    "        # _______ concatenate them together ______\n",
    "        nonzero_matrix = torch.cat((nonzero_matrix_ui, nonzero_matrix_preference), dim=1)\n",
    "        feature_bias_matrix = torch.cat((feature_bias_matrix_ui, feature_bias_matrix_preference), dim=1)\n",
    "\n",
    "        # _______ make a clone _______\n",
    "        nonzero_matrix_clone = nonzero_matrix.clone()#uid and idd and preference embedding\n",
    "        feature_bias_matrix_clone = feature_bias_matrix.clone()#bias matrix\n",
    "        \n",
    "        #Second-order term of FM:\n",
    "\n",
    "        # _________ sum_square part _____________\n",
    "        summed_features_embedding_squared = nonzero_matrix.sum(dim=1, keepdim=True) ** 2  # (bs, 1, emb_size) it's the first half of the pairwise interaction term in the FM formula.\n",
    "\n",
    "        # _________ square_sum part _____________\n",
    "        squared_sum_features_embedding = (nonzero_matrix * nonzero_matrix).sum(dim=1, keepdim=True)  # (bs, 1, emb_size)\n",
    "\n",
    "        # ________ FM __________\n",
    "        FM = 0.5 * (summed_features_embedding_squared - squared_sum_features_embedding)  # (bs, 1, emb_size)\n",
    "\n",
    "        # Optional: remove the inter-group interaction\n",
    "        # ***---***\n",
    "        #算出每个user的preference的FM\n",
    "        new_non_zero_2 = nonzero_matrix_preference\n",
    "        summed_features_embedding_squared_new_2 = new_non_zero_2.sum(dim=1, keepdim=True) ** 2\n",
    "        squared_sum_features_embedding_new_2 = (new_non_zero_2 * new_non_zero_2).sum(dim=1, keepdim=True)\n",
    "        newFM_2 = 0.5 * (summed_features_embedding_squared_new_2 - squared_sum_features_embedding_new_2)\n",
    "        FM = (FM - newFM_2)\n",
    "        #The intention of this is to make the model focus more on the interactions between different groups (like user-item, user-preference, item-preference)\n",
    "        #rather than interactions within the same group (like preference-preference). \"\"\"\n",
    "        # ***---***\n",
    "\n",
    "        FM = self.dropout2(FM)  # (bs, 1, emb_size)\n",
    "\n",
    "        Bilinear = FM.sum(dim=2, keepdim=False)  # (bs, 1)\n",
    "        result = Bilinear + self.Bias  # (bs, 1)\n",
    "        #result is the predicted score\n",
    "        return result, feature_bias_matrix_clone, nonzero_matrix_clone\n",
    "    # end def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bcd50-f683-46d7-a0ff-35c98f8bcaf8",
   "metadata": {},
   "source": [
    "# Fedmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da648fe5-31cb-416f-9cdf-ff0391716e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_pickle_to_data(dataset, kg, pickle_file, iter_, bs, pickle_file_length, uf):\n",
    "    '''\n",
    "    user_pickle = pickle_file[0]\n",
    "    item_p_pickle = pickle_file[1]\n",
    "    i_neg1_pickle = pickle_file[2]\n",
    "    i_neg2_pickle = pickle_file[3]\n",
    "    preference_pickle = pickle_file[4]\n",
    "    '''\n",
    "    left, right = iter_ * bs, min(pickle_file_length, (iter_ + 1) * bs)\n",
    "    # user_pickle, item_p_pickle, i_neg1_pickle, i_neg2_pickle, preference_pickle = zip(*pickle_file[left:right])\n",
    "\n",
    "    pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_2 = [], [], [], [], [], [], [], []\n",
    "\n",
    "    I = pickle_file[0][left:right]\n",
    "    II = pickle_file[1][left:right]\n",
    "    III = pickle_file[2][left:right]\n",
    "    IV = pickle_file[3][left:right]\n",
    "    V = pickle_file[4][left:right]\n",
    "\n",
    "    residual_feature, neg_feature = None, None\n",
    "\n",
    "    if uf == 1:\n",
    "        feature_range = np.arange(feature_length).tolist()\n",
    "        residual_feature, neg_feature = [], []\n",
    "        for user_pickle, item_p_pickle, i_neg1_pickle, i_neg2_pickle, preference_pickle in zip(I, II, III, IV, V):\n",
    "            gt_feature = kg.G[ITEM][item_p_pickle][ITEM_FEATURE]\n",
    "            this_residual_feature = list(set(gt_feature) - set(preference_pickle))\n",
    "            remain_feature = list(set(feature_range) - set(gt_feature))\n",
    "            this_neg_feature = np.random.choice(remain_feature, len(this_residual_feature))\n",
    "            residual_feature.append(torch.LongTensor(this_residual_feature))\n",
    "            neg_feature.append(torch.LongTensor(this_neg_feature))\n",
    "        residual_feature = pad_sequence(residual_feature, batch_first=True, padding_value=PAD_IDX2)\n",
    "        neg_feature = pad_sequence(neg_feature, batch_first=True, padding_value=PAD_IDX2)\n",
    "\n",
    "    i = 0\n",
    "    index_none = list()\n",
    "    for user_pickle, item_p_pickle, i_neg1_pickle, i_neg2_pickle, preference_pickle in zip(I, II, III, IV, V):\n",
    "        pos_list.append(torch.LongTensor([user_pickle, item_p_pickle + user_length]))\n",
    "        f = kg.G[ITEM][item_p_pickle][ITEM_FEATURE]\n",
    "        pos_list2.append(torch.LongTensor(f))\n",
    "        neg_list.append(torch.LongTensor([user_pickle, i_neg1_pickle + user_length]))\n",
    "        f = kg.G[ITEM][i_neg1_pickle][ITEM_FEATURE]\n",
    "        neg_list2.append(torch.LongTensor(f))\n",
    "\n",
    "        preference_list_1.append(torch.LongTensor(preference_pickle))\n",
    "        if i_neg2_pickle is None:\n",
    "            index_none.append(i)\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    for user_pickle, item_p_pickle, i_neg1_pickle, i_neg2_pickle, preference_pickle in zip(I, II, III, IV, V):\n",
    "        if i in index_none:\n",
    "            i += 1\n",
    "            continue\n",
    "        new_neg_list.append(torch.LongTensor([user_pickle, i_neg2_pickle + user_length]))\n",
    "        f = kg.G[ITEM][i_neg2_pickle][ITEM_FEATURE]\n",
    "        new_neg_list2.append(torch.LongTensor(f))\n",
    "        preference_list_2.append(torch.LongTensor(preference_pickle))\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    pos_list = pad_sequence(pos_list, batch_first=True, padding_value=PAD_IDX1)\n",
    "    pos_list2 = pad_sequence(pos_list2, batch_first=True, padding_value=PAD_IDX2)\n",
    "    neg_list = pad_sequence(neg_list, batch_first=True, padding_value=PAD_IDX1)\n",
    "    neg_list2 = pad_sequence(neg_list2, batch_first=True, padding_value=PAD_IDX2)\n",
    "    new_neg_list = pad_sequence(new_neg_list, batch_first=True, padding_value=PAD_IDX1)\n",
    "    new_neg_list2 = pad_sequence(new_neg_list2, batch_first=True, padding_value=PAD_IDX2)\n",
    "    preference_list_1 = pad_sequence(preference_list_1, batch_first=True, padding_value=PAD_IDX2)\n",
    "    preference_list_2 = pad_sequence(preference_list_2, batch_first=True, padding_value=PAD_IDX2)\n",
    "\n",
    "    if uf != 0:\n",
    "        return cuda_(pos_list), cuda_(pos_list2), cuda_(neg_list), cuda_(neg_list2), cuda_(new_neg_list), cuda_(\n",
    "            new_neg_list2), cuda_(preference_list_1), cuda_(preference_list_2), index_none, cuda_(residual_feature), cuda_(neg_feature)\n",
    "    else:\n",
    "        return cuda_(pos_list), cuda_(pos_list2), cuda_(neg_list), cuda_(neg_list2), cuda_(new_neg_list), cuda_(\n",
    "            new_neg_list2), cuda_(preference_list_1), cuda_(preference_list_2), index_none, residual_feature, neg_feature\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937aa0cd-713c-4b5b-b7d1-148696e80f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open pickle file: train_fm_data takes 0.08383893966674805 seconds\n",
      "Starting 0 epoch\n"
     ]
    }
   ],
   "source": [
    "server = FedRecServer(hs, user_length, item_length, feature_length, qonly, hs, ip, dr).to(device)\n",
    "client_dic={}\n",
    "lsigmoid = nn.LogSigmoid()\n",
    "reg_float = float(reg)\n",
    "for epoch in range(load_fm_epoch, max_epoch+1):\n",
    "        #一开始先train，run this first\n",
    "        # _______ Do the evaluation _______\n",
    "        #if epoch % observe == 0 and epoch > -1:#0,25,50...\n",
    "            #print('Evaluating on feature similarity')      #要改这里\n",
    "            #evaluate_feature(kg, model, epoch, filename, PAD_IDX1, PAD_IDX2, user_length, feature_length, data_name, ITEM, ITEM_FEATURE)#call the function from FM_feature_evaluate.py\n",
    "            #print('Evaluating on item similarity')\n",
    "            #evaluate_item(kg, model, epoch, filename, 0, PAD_IDX1, PAD_IDX2, user_length, feature_length, data_name, ITEM, ITEM_FEATURE)\n",
    "\n",
    "        tt = time.time()#record the operation time\n",
    "        pickle_file = load_fm_sample(dataset=data_name, mode='train', epoch=epoch % 50)# pick up the train set from utils.py  pickle file 里面有五个list\n",
    "\n",
    "        print('Open pickle file: train_fm_data takes {} seconds'.format(time.time() - tt))\n",
    "        pickle_file_length = len(pickle_file[0])\n",
    "        \n",
    "        mix = list(zip(pickle_file[0], pickle_file[1], pickle_file[2], pickle_file[3], pickle_file[4]))\n",
    "        random.shuffle(mix)\n",
    "        I, II, III, IV, V = zip(*mix)\n",
    "        new_pk_file = [I, II, III, IV, V]\n",
    "        start = time.time()\n",
    "        print('Starting {} epoch'.format(epoch))\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_2 = 0\n",
    "        max_iter = int(pickle_file_length / float(bs))\n",
    "        for iter_ in range(max_iter):#逐批逐批的将batch取出来\n",
    "            if iter_ > 1 and iter_ % 1000 == 0:\n",
    "                print('--')\n",
    "                print('Takes {} seconds to finish {}% of this epoch'.format(str(time.time() - start),\n",
    "                                                                            float(iter_) * 100 / max_iter))\n",
    "                print('loss is: {}'.format(float(epoch_loss) / (bs * iter_)))\n",
    "                print('iter_:{} Bias grad norm: {}, Static grad norm: {}, Preference grad norm: {}'.format(iter_, torch.norm(model.Bias.grad), torch.norm(model.ui_emb.weight.grad), torch.norm(model.feature_emb.weight.grad)))\n",
    "\n",
    "            pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_new, index_none, residual_feature, neg_feature \\\n",
    "                = translate_pickle_to_data(dataset, kg, new_pk_file, iter_, bs, pickle_file_length, uf)\n",
    "\n",
    "            #每个batch的每个uid都train\n",
    "            for i in range(len(pos_list)):\n",
    "                user_id = pos_list[i][0].item()  # Extract user id and convert tensor to python scalar\n",
    "                if user_id not in client_dic:\n",
    "                    client_dic[user_id] = FedRecClient(hs, user_length, item_length, feature_length, qonly, hs, ip, dr).to(device)\n",
    "\n",
    "                    #break#!!!!!!!!!!!!!!!!!!!!!!\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            batch_loss=0\n",
    "            batch_loss_2=0\n",
    "            \n",
    "            #user_id = pos_list[i][0].item()\n",
    "            batch_loss,batch_loss_2=server.train_(client_dic,pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_new, index_none, residual_feature, neg_feature)\n",
    "                \n",
    "            epoch_loss += batch_loss\n",
    "            epoch_loss_2 +=batch_loss_2\n",
    "            \n",
    "            #batch_loss=0\n",
    "            #batch_loss_2=0\n",
    "            #for i in range(len(pos_list)):\n",
    "                #user_id = pos_list[i][0].item()\n",
    "                #loss,loss_2=server.train_(client_dic[user_id],pos_list[i], pos_list2[i], neg_list[i], neg_list2[i], new_neg_list[i], new_neg_list2[i], preference_list_1[i], preference_list_new[i], index_none, residual_feature[i], neg_feature[i])\n",
    "                #batch_loss+=loss\n",
    "                #batch_loss_2+=loss_2\n",
    "            #epoch_loss += batch_loss\n",
    "            #epoch_loss_2 +=loss_2\n",
    "            \n",
    "                #break#!!!!!!!!\n",
    "            #break#!!!!!!!!!!!!!\n",
    "        print('epoch loss: {}'.format(epoch_loss / pickle_file_length))\n",
    "        print('2ND loss is: {}'.format(float(epoch_loss_2) / (bs * iter_)))\n",
    "        #break#!!!!!!!!!!!!\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29a3a8-af3c-4fb8-9d9a-d6b912e64c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = load_fm_sample(dataset=data_name, mode='train', epoch=1)# pick up the train set from utils.py  pickle file 里面有五个list\n",
    "\n",
    "\n",
    "pickle_file_length = len(pickle_file[0])\n",
    "        \n",
    "mix = list(zip(pickle_file[0], pickle_file[1], pickle_file[2], pickle_file[3], pickle_file[4]))\n",
    "random.shuffle(mix)\n",
    "I, II, III, IV, V = zip(*mix)\n",
    "new_pk_file = [I, II, III, IV, V]\n",
    "pos_list, pos_list2, neg_list, neg_list2, new_neg_list, new_neg_list2, preference_list_1, preference_list_new, index_none, residual_feature, neg_feature \\\n",
    "                = translate_pickle_to_data(dataset, kg, new_pk_file, 1, bs, pickle_file_length, uf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acc53b-c814-47b4-b0f3-5a6f1071741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic={}\n",
    "for i in range(len(pos_list)):\n",
    "                user_id = pos_list[i][0].item()  # Extract user id and convert tensor to python scalar\n",
    "                if user_id not in client_dic:\n",
    "                    client_dic[user_id] = FedRecClient(hs, user_length, item_length, feature_length, qonly, hs, ip, dr,pos_list[0], pos_list2[0], neg_list[0], neg_list2[0], new_neg_list[0], new_neg_list2[0], preference_list_1[0], preference_list_new[0], index_none, residual_feature[0], neg_feature[0]).to(device)\n",
    "\n",
    "                    #break#!!!!!!!!!!!!!!!!!!!!!!\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6031446-02f3-4e89-843f-a81f28150409",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=0\n",
    "for i in client_dic.keys():\n",
    "    uid=i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af438e-fdcc-431f-9452-4c5842acb2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = FedRecServer(hs, user_length, item_length, feature_length, qonly, hs, ip, dr).to(device)\n",
    "client_dic[uid].model(server.items_emb,server.feature_emb,pos_list[0].unsqueeze(0),pos_list2[0].unsqueeze(0),preference_list_1[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff1da-64d9-4942-846d-f6db9a7f9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9310b7-fc85-4923-8ca3-bcaa8c58211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic[uid].model.ui_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b4106-be26-4acb-aaa2-2beb1339b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list[0][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ea64d-906d-4469-97af-ee91ce0bef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic[uid].model.ui_emb[0][0].unsqueeze(0)[..., :-1].unsqueeze(dim=1).detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d65d33-4d88-49bf-8e82-5aec5594a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_list_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd999a-f312-47ce-98ce-f1e589aa4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic[uid].model.feature_emb(preference_list_1[0].unsqueeze(0))[..., :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf1d79-f452-452e-be9f-b7634fdf53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_feature[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7b262-1ff9-4940-9459-bb5ec9182b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3468f17-6a89-4daf-9cf7-63f35ccc4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic[475].model.feature_emb(residual_feature)[..., :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2006087-08e5-4f8b-ab41-010fde812058",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dic={}\n",
    "uid=0\n",
    "for i in range(len(pos_list)):\n",
    "    user_id = pos_list[i][0].item()  # Extract user id and convert tensor to python scalar\n",
    "    if user_id not in client_dic:\n",
    "        client_dic[user_id] = FedRecClient(hs, user_length, item_length, feature_length, qonly, hs, ip, dr,pos_list[0], pos_list2[0], neg_list[0], neg_list2[0], new_neg_list[0], new_neg_list2[0], preference_list_1[0], preference_list_new[0], index_none, residual_feature[0], neg_feature[0]).to(device)\n",
    "        uid=user_id\n",
    "        break\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4290219-ea7a-4ee9-a056-e523866eef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "\n",
    "items_emb = nn.Embedding(item_length, hs+1).to(device)\n",
    "nn.init.normal_(items_emb.weight, std=0.01)\n",
    "        \n",
    "feature_emb = nn.Embedding(feature_length+1, hs + 1, padding_idx=feature_length, sparse=False).to(device)\n",
    "feature_emb.weight.data.normal_(0,ip)\n",
    "\n",
    "        # _______ set the padding to zero _______\n",
    "feature_emb.weight.data[feature_length,:] = 0\n",
    "client_dic[uid].train_(items_emb,feature_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d91228-7c7b-4398-8168-0e123c4f0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param1, param2 = list(), list()\n",
    "param3 = list()\n",
    "\n",
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "        print(name, param)\n",
    "        if i == 0:\n",
    "            param1.append(param)\n",
    "        else:\n",
    "            param2.append(param)\n",
    "        if i == 2:\n",
    "            param3.append(param)\n",
    "        i += 1\n",
    "if optim == 'Ada':\n",
    "        optimizer1 = torch.optim.Adagrad(param1, lr=args.lr, weight_decay=args.decay)\n",
    "        optimizer2 = torch.optim.Adagrad(param2, lr=args.lr, weight_decay=args.decay)\n",
    "        optimizer3 = torch.optim.Adagrad(param3, lr=args.flr, weight_decay=args.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7843828-338d-481c-8b92-4f2925468cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
